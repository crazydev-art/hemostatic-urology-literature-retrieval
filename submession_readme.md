# Query System Optimization and Performance Enhancement

## Approach to Solving Each Task

---
## ‚úÖ Requirements
- requests
- tenacity
- ratelimit
- python-dotenv
- pytest
- tqdm
> ‚úÖ **Make sure to enter your API key in the `.env` file** to avoid hitting public rate limits.
please replace this variable in the .env file < NCBI_API_KEY = 'Your_API_KEY'> by your personal API key 

### ‚úÖ Task 1: Query Optimization

**Objective:**  
Address "Request URI too long" errors and use full datasets (`HEMOSTATIC_DEVICES_FLAT`, `UROLOGY_INDICATORS_FLAT`).

**Approach:**

- Implemented `split_terms` in `query_builder.py` to split large queries into batches (`batch_size=1000`) based on character length, ensuring each batch‚Äôs URL is under `max_url_length=2000`.
- Alternated device and indicator terms to balance batches while preserving query logic:  
  `(devices) AND (indicators) [AND date]`.
- Updated `main.py` to use full datasets, generating ~4 batches for 81 device and 137 indicator terms mentioned in the config file.

**Outcome:**  
‚úÖ Eliminated HTTP 414 errors, enabling robust query processing for large datasets.

---

### üöÄ Task 2: Performance Enhancement

**Objective:**  
Implement batching, pagination, parallel processing, and retry logic for large result sets (10,000+ articles).

**Approach:**

- **Batching:**  
  `split_terms` creates balanced batches, reducing API calls (e.g., 4 batches instead of 1 large query).
- **Pagination:**  
  `fetch_summary` in `api_client.py` uses `retstart` and `retmax=1000` to fetch results in chunks.
- **Parallel Processing:**  
  `batch_search` in `search.py` uses `ThreadPoolExecutor` (2 workers) to process batches concurrently.
- **Retry Logic:**  
  `process_batch` retries failed batches once; `_make_request` uses `ratelimit` to throttle requests:  
  - 10/sec with API key  
  - 3/sec without API key  
  Prevents 429 errors.

**Outcome:**  
‚úÖ Achieved ~623 results in ~4.38 seconds (from logs), with efficient batching and pagination.  
‚ö†Ô∏è Partial results (e.g., 174/11,174 records) indicate room for enhanced retry mechanisms.

---

### üéÅ Bonus: Progress Tracking

**Objective:**  
Add progress bars for long-running operations.

**Approach:**

- Integrated `tqdm` in `batch_search` (tracks batch processing) and `fetch_summary` (tracks pagination).
- Example outputs:
  - `Processing batches: 75% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3/4 [00:03<00:01, 1.19s/it]`
  - `Fetching summaries: 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:03<00:00, 3.30it/s]`

**Outcome:**  
‚úÖ Enhanced user experience through visual progress tracking.

---

### üßæ Bonus: Logging

**Objective:**  
Replace `print` statements with proper logging.

**Approach:**

- Configured `logging` in `main.py` and core modules.
- Used levels: `INFO`, `WARNING`, `ERROR`.
- Tracked:
  - Successes (e.g., ‚ÄúFetched 450 records‚Äù)
  - Failures (e.g., ‚ÄúBatch failed‚Äù)
  - Partial results (e.g., ‚ÄúFetched 174 of 11,174 records‚Äù)

**Outcome:**  
‚úÖ Improved debugging and monitoring. All `print` statements replaced.

---

### ‚úÖ Bonus: Testing

**Objective:**  
Ensure correctness through unit tests.

**Approach:**

- Added `tests/test_search.py` with 12 tests covering:
  - `create_query`
  - `split_terms`
  - `batch_search`
  - `search_pmc`
  - `search_pubmed_pmc`
- Covered:
  - Valid cases
  - Edge cases (empty/invalid queries)
  - API failure scenarios (mocked via `unittest.mock`)

**Outcome:**  
‚úÖ Verified correctness and robustness. No real API calls required for testing.

---

## üîß Design Decisions and Rationale

- **Modular Codebase:**  
  Split into `config.py`, `api_client.py`, `query_builder.py`, `search.py`, `utils.py`, `tests/`  
  ‚Üí Improves maintainability and extensibility.
- **Dynamic Rate Limiting:**  
  Used `ratelimit` to stay within NCBI API rules.
- **Limited Concurrency:**  
  Used 2 parallel workers (safe default for rate limits).
- **Error Recovery:**  
  Skips failed batches and logs issues to ensure maximum data retrieval.
- **Progress UI:**  
  Used `tqdm` for terminal-friendly visual feedback.
- **Test Coverage:**  
  Comprehensive unit tests for stability and reliability.

---

## üìä Performance Metrics

- **Timing:**  
  Retrieved ~623 results from 4 batches in ~4.38 seconds.
- **Pagination:**  
  Handled large result sets (e.g., 12 iterations for 11,174 results).
- **Progress Tracking:**  
  Tracked 4 batches and 12 pagination calls with progress bars.

---

## ‚úÖ Improvements Summary

### Task 1: URI Optimization
- ‚úÖ Split long queries into ~4 batches
- ‚úÖ Avoided HTTP 414 (URI Too Long) errors

### Task 2: Large Dataset Handling
- ‚úÖ Efficient batching
- ‚úÖ Paginated retrieval
- ‚úÖ Parallel batch processing (2 threads)
- ‚úÖ Rate-limited API calls to avoid 429 errors
- ‚úÖ Partial retry mechanism for failed batches

### Bonuses
- ‚úÖ Visual progress bars (`tqdm`)
- ‚úÖ Structured logging
- ‚úÖ Unit test coverage

---

## ‚ö†Ô∏è Known Limitations

- Partial results in some cases (e.g., 174/11,174 records)
- Current retry logic is basic; future improvement with `tenacity` or exponential backoff.

---

## üìÇ Folder Structure

project/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ ‚îú‚îÄ‚îÄ api_client.py
‚îÇ ‚îú‚îÄ‚îÄ query_builder.py
‚îÇ ‚îú‚îÄ‚îÄ search.py
‚îÇ ‚îî‚îÄ‚îÄ utils.py
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ ‚îî‚îÄ‚îÄ test_search.py
‚îÇ
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ main.py
‚îî‚îÄ‚îÄ README.md
